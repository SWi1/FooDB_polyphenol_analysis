---
title: "06_FooDB_FNNDSFullMatch_Part3"
author: "Stephanie Wilson"
date: "`r Sys.Date()`"
output: html_document
---

#  Matching FooDB to ASA24 Ingredient Descriptions
## Step 6: Matching ASA24 to FooDB
## Part 3: Text Similarity and CHO Matching for missing code matches

__Required Input Files__  
  - *asa_foodb_descrip_dependencies.csv* - Output from 05_FooDB_FNDDS_FullMatch_Part2
  - *2017-2018 FNDDS At A Glance - Ingredient Nutrient Values.xlsx* - downloaded from [BHNRC database](https://www.ars.usda.gov/northeast-area/beltsville-md-bhnrc/beltsville-human-nutrition-research-center/food-surveys-research-group/docs/fndds-download-databases/)
  - *Content_updated.csv.bz2* - output from 01_FooDB_FooDBCleaning.ipynb
  - *Food_V2_descripcleaned.csv* - output from 02_FooDB_TextProcessing
  
__Information__. 
This script utilizes dependency data (Script 05) and forms an ASA-based descriptor search term. Each search term is checked for possible food descriptions matches in FooDB. The top five most similar texts are filtered in. Any perfect matches (text similarity score of 1) are automatically filtered. The remaining matches are run through a carbohydrate macronutrient check. 

  1) Isolates and prepares dependency descriptor search term 
    a) Isolate dependency descriptor using a laddered if else loop.
    b) Create grepl search phrase of filtered dependency terms
  2) Grepl search of FooDB using descriptor terms.
  3) Match Macronutrient similarity
    a) Prepare FoodB and ASA Macronutrient files
    b) Filtering based on CHO and text similarity
    c) Merge and Export
  
__Output__
  - *asa_foodb_descrip_dependencies_updated.csv* - An updated version of the dependency file 
  - *foodb_macros_g.csv* - Macronutrient + Iron content in grams from FooDB Content file. Can be used for nutrient matching for any ingredient sets.
  - *missing_foodb_descrip.csv* - Of the 170 original missing codes, this file includes now perfect text matches (28) AND text + CHO matches (110). Still missing are the codes that did not have a grepl search (29). 


```{r Load packages, include= FALSE, warning=FALSE, message=FALSE}
library(tidyverse); library(stringdist); library(readxl); library(reshape2)
```


```{r Load Data, include=FALSE}
asa_dependency = read.csv("data/asa_foodb_descrip_dependencies.csv", header=TRUE)

fndds = read_xlsx('data/2017-2018 FNDDS At A Glance - Ingredient Nutrient Values.xlsx', skip = 1) %>%
  filter(`Nutrient description` %in% c('Protein', 'Total Fat', 'Carbohydrate')) %>%
  select(c(`Ingredient code`, `Nutrient description`, `Nutrient value`))

Content = read.csv('FooDB/Content_updated.csv.bz2')
Food_V2 = read.csv('FooDB/Food_V2_descripcleaned.csv', header =TRUE)
```


```{r Additional data tidying}
#replaces spaces in FNDDS column names with underscores
colnames(fndds) = gsub(' ', '_', colnames(fndds)) 

#Replace common name with cleaned common name
Content[["foodb_descrip"]] = Food_V2[match(Content[['food_V2_ID']],
                                                    Food_V2[['food_V2_ID']]), 
                                              'orig_food_common_name']
```


We will match ASA24 ingredient descriptions to FooDB through a combination of text description and macronutrient similarity.


### 1) Isolate and prepare dependency descriptor search term 

If column names are not in the following order, they must be relocated to this order for accurate term searching. This term searching was manually determined from Script 05, Part 2.

  - compound > nsubjpass > nmod > nsubj > amod > npadvmod > ROOT
  
```{r Subset the dependency data}
POS = asa_dependency[, 6:12]
colnames(POS)
```

#### 1a) Isolate dependency descriptor using a laddered if else loop.
The following ladder loop detects if the dependency column contains any words. If it does, then those words are filtered in. If it does not contain words, the next dependency is checked for the presence of any words. Downstream dependencies after **TRUE** are not assessed for that ingredient description.

```{r laddered loop to extract descriptor term}
output = vector('character', nrow(POS))

for (i in 1:nrow(POS)) {
  if(str_detect(POS[i, ]$compound, '[:alpha:]')==TRUE) {
    output[[i]] = POS[i, ]$compound
   } else { 
     if(str_detect(POS[i, ]$nsubjpass, '[:alpha:]')==TRUE) {
         output[[i]] = POS[i, ]$nsubjpass
       } else {
         if(str_detect(POS[i, ]$nmod, '[:alpha:]')==TRUE) {
             output[[i]] = POS[i, ]$nmod
         } else {
           if(str_detect(POS[i, ]$nsubj, '[:alpha:]')==TRUE) {
             output[[i]] = POS[i, ]$nsubj
           } else {
             if(str_detect(POS[i, ]$amod, '[:alpha:]')==TRUE) {
             output[[i]] = POS[i, ]$amod
                 } else {
                 if(str_detect(POS[i, ]$npadvmod, '[:alpha:]')==TRUE) {
                 output[[i]] = POS[i, ]$npadvmod
                     } else {
                     if(str_detect(POS[i, ]$ROOT, '[:alpha:]')==TRUE) {
                     output[[i]] = POS[i, ]$ROOT
                  }
                }
              }
            }
          }
        }
     }
}
```


The following cleans (punctuation, tokenizing, prepare as df) the dependency terms that were filtered in.
```{r warning = FALSE}
# Remove punctuation
descriptor = gsub('[[:punct:]]', '', output)

#Split the string into separate words. 
descriptor = sapply(strsplit(as.character(descriptor), split = " "), trimws)

# Create a dataframe of descriptor terms
# If there are 1+ words in the dependency, each word gets their own column.
# If there's only 1 word in the dependency, the word is repeated across columns.
descriptor.df = as.tibble(do.call(rbind, descriptor))
```


#### 1b) Create grepl search phrase of filtered dependency terms
Words that have been repeated (from 1 word in the dependency) are removed. If there are more than 1 unique words in the dependency, a unique search term is created with the first two words. 
```{r Create grepl search}
searchterm = vector('character', nrow(descriptor.df))

for (i in 1:nrow(descriptor.df)) {
  if(descriptor.df[i, ]$V1 == descriptor.df[i,]$V2) {
    searchterm[[i]] = paste(descriptor.df[i, ]$V1, sep = '')
   } else {
     searchterm[[i]] = paste(descriptor.df[i, ]$V1, ".*", descriptor.df[i, ]$V2, 
                         "|", descriptor.df[i, ]$V2, ".*", descriptor.df[i, ]$V1, 
                         sep = '')
   }
}
```


```{r Create grepl search for secondary matching in script 07}
searchterm2 = vector('character', nrow(descriptor.df))

for (i in 1:nrow(descriptor.df)) {
  if(descriptor.df[i, ]$V1 == descriptor.df[i,]$V2) {
    searchterm2[[i]] = paste(descriptor.df[i, ]$V1, sep = '')
   } else {
     searchterm2[[i]] = paste(descriptor.df[i, ]$V1, "|", descriptor.df[i, ]$V2, 
                         sep = '')
   }
}
```


```{r}
#Add search terms to asa_dependency dataframe
asa_dependency_updated = asa_dependency %>%
  mutate(searchterm = searchterm, 
         searchterm2 = searchterm2) 

# Prepare missing FooDB description dataframe 
missing = asa_dependency_updated %>%
  filter(is.na(orig_food_id)) %>%
  select(c(Ingredient_code, Ingredient_description, searchterm, searchterm2))

# Export the dependency file now updated with search terms.
asa_dependency_updated = asa_dependency_updated %>%
  select(-c(orig_food_id, orig_food_common_name, food_V2_ID))

# Save for downstream use
write.csv(asa_dependency_updated, 'data/asa_foodb_descrip_dependencies_updated.csv')
```


### 2) Grepl search of FooDB using descriptor terms 

Load function for downstream filtering.
```{r text similarity function}
sim_score <- function(x, asa_descrip, foodb_descrip){
  metric = x #where x is a list of text similarity metrics accepted by stringsim
  asa_descrip = asa_descrip #df column of asa ingredient descriptions
  foodb_descrip = foodb_descrip #df column of foodb ingredient descriptions
  output = data.frame(matrix(ncol=length(metric), nrow=length(asa_descrip)))

#run text similarity with each metric  
for(i in seq_along(metric)){ 
  output[[i]] = stringsim(asa_descrip, foodb_descrip, 
                          method = metric[[i]], p = 0.1)
}
  
  colnames(output) = metric # rename column names
  output = output %>% add_column(asa_descrip, foodb_descrip) %>% # add columns
    relocate(asa_descrip, foodb_descrip) # relocate descriptions to front
  
  return(output)
}
```


Looping the grepl search through each search term and adding resulting output to a dataframe for macronutrient checking. 
```{r grepl search, warning = FALSE}
finaloutput = data.frame()

for(i in 1:nrow(missing)){
  
# Part I - filter by pattern search
input = Food_V2 %>%
  select(food_V2_ID, orig_food_common_name) %>%
  filter(grepl(missing[i, ]$searchterm, orig_food_common_name, ignore.case = TRUE))

# Part II - Run function to calculate multiple text similarity scores
scores_auto = sim_score(x = c("osa", "lv", "dl", "hamming", "lcs", 
                              "qgram", "cosine", "jaccard", "jw", "soundex"), 
                        asa_descrip = rep(missing[i, ]$Ingredient_description, times = nrow(input)), 
                        foodb_descrip = input$orig_food_common_name)

# Part III - Add FooDB ID column
scores_auto$food_V2_ID = input$food_V2_ID

## Part IV - Calculate Mean of text similarity indices
scores_auto = scores_auto %>% mutate(scores_auto, mean_score = 
           rowMeans(select(scores_auto, c(osa, lv, dl, hamming, lcs, 
                                          qgram, cosine, jaccard, jw, soundex)))) 

# Part V - Filter in top three scores for macronutrient comparison
# Highest score given larger number
output = scores_auto %>%
  top_n(mean_score, n = 5) %>%
  mutate(text_rank = dense_rank(desc(mean_score)))

finaloutput = rbind(finaloutput, output)
}
```


```{r}
message('Number of unique codes that DID NOT come through the grepl search: ', 
        nrow(missing) - length(unique(finaloutput$asa_descrip)))
```

We can see that 45 codes did not come up with any search hits. We will revisit what is going on with these codes after we have found a matches for the codes that *did* have hits. 

For now, let us update the final output file with IDs. FooDB ids were already added as part of the above loop.

```{r}
# remove everything but the descriptions and mean score.
finaloutput1 = finaloutput %>% 
  select(c(asa_descrip, foodb_descrip, mean_score, food_V2_ID, text_rank)) %>%
  rename(c('Ingredient_description' = 'asa_descrip', 
           'orig_food_common_name' = 'foodb_descrip'))

# Prepare ASA IDs
finaloutput.ASA = missing %>%
  select(c('Ingredient_code', 'Ingredient_description'))

# Add ASA IDs
finaloutput.updated = left_join(finaloutput1, finaloutput.ASA, 
                         by = 'Ingredient_description') %>%
  relocate('Ingredient_code', 'Ingredient_description',
           'orig_food_common_name', 'food_V2_ID', .before = mean_score)
```


Anything with a perfect similarity score can be filtered in without macronutrient matching. We will add these to the remaining descriptions downstream for replacing in the ASA file. 
```{r}
perfectmatch = finaloutput.updated %>% 
  filter(mean_score == 1)
# We have 28 perfect matches

#Let us take out the Ingredient Codes with perfect matches 
#for now so we can move on to macro matching
finaloutput.updated = finaloutput.updated %>% 
  filter(!Ingredient_code %in% perfectmatch$Ingredient_code)

message('Number of unique codes that require macronutrient matching: ', length(unique(finaloutput.updated$Ingredient_code)))
```


### 3) Match Carbohydrate similarity

#### 3a) Prepare FoodB and ASA Macronutrient files
This code will also filter in the remaining descriptions (anything that wasn't a perfect text match).

Prepare FooDB Content by filtering in macronutrients and iron. Filter based on source_id:
  1 = Fat  
  2 = Protein  
  3 = Carbohydrates
  16258 = Iron
  
```{r Filter Macronutrients}
foodb_macros = filter(Content, source_id %in% c('1', '2', '3', '16258')) %>% 
  filter(!is.na(orig_content)) %>%
  mutate(Nutrient = recode_factor(source_id,
                               `1` = "FAT", `2` = "PRO", `3` = "CHO", 
                               `16258` = 'Iron'))
```

ASA24 Macronutrient Data is provided as grams, so in order to do a macronutrient comparison we need to ensure proper units. It would not be appropriate to average across food_V2_id as some entries are related to specific parts of the food (Ex. seed vs leaf vs fruit).

```{r Prepare FooDB}
# Check unit similarity
foodb_macros %>% count(source_id, orig_unit)
```


```{r}
# Create new value column in gram
foodb_macros_g = foodb_macros %>%
  mutate(orig_content_g = orig_content/1000, source_id = factor(source_id)) %>%
  select(c(id, food_V2_ID, foodb_descrip, Nutrient, orig_content_g)) %>%
  rename('FooDB_Nutrient' = 'Nutrient', 'FooDB_Value' = 'orig_content_g') 

#Export for secondary use
#Contains nutrient information on 8354 unique FooDB V2 IDs
write.csv(foodb_macros_g, 'FooDB/foodb_macros_g.csv', row.names = FALSE)

#Filter in IDs from our output
foodb_macros_g = foodb_macros_g %>%
  filter(food_V2_ID %in% finaloutput.updated$food_V2_ID)
```


ASA nutrient values are the amounts per 100 grams edible portion
```{r Merge ASA missing food items with FNDDS nutrient values}
asa_macros = missing %>%
  select(c('Ingredient_code', 'Ingredient_description')) %>%
             left_join(fndds, missing, by = 'Ingredient_code') %>%
  rename('ASA_Macro' = 'Nutrient_description',
         'ASA_Value' = 'Nutrient_value') %>%
  filter(Ingredient_code %in% finaloutput.updated$Ingredient_code)
```


```{r message = FALSE}
#Merge to-match ASA Ingredient Descriptions with their FNDDS macronutrient profiles.
Macro_matching = left_join(finaloutput.updated, asa_macros, 
                           by = 'Ingredient_code',
                           keep= FALSE) %>%
  select(-Ingredient_description.y) %>%
  rename('Ingredient_description' = 'Ingredient_description.x')

Macro_matching =  left_join(Macro_matching, foodb_macros_g) %>%
  select(-c(id, foodb_descrip))
```

Generate Macronutrient Subsets.  
Generate a rank based on how close FooDB nutrient values are to ASA nutrient values.  
  - Lower scores go to *most* similar values.
  - Filter based off of lowest scores
```{r}
FAT = Macro_matching %>% 
  filter(ASA_Macro =='Total Fat') %>% 
  filter(FooDB_Nutrient == 'FAT')  %>% 
  group_by(Ingredient_code) %>%
  mutate(FAT_Rank = dense_rank(abs(FooDB_Value - ASA_Value))) %>%
  select(c(Ingredient_code, food_V2_ID, mean_score, text_rank, FAT_Rank)) 
  
CHO = Macro_matching %>% 
  filter(ASA_Macro =='Carbohydrate') %>% 
  filter(FooDB_Nutrient == 'CHO') %>% 
  group_by(Ingredient_code) %>%
  mutate(CHO_Rank = dense_rank(abs(FooDB_Value - ASA_Value)))  %>%
  select(c(Ingredient_code, food_V2_ID, mean_score, text_rank, CHO_Rank)) 

PRO = Macro_matching %>% 
  filter(ASA_Macro =='Protein') %>% 
  filter(FooDB_Nutrient== 'PRO')  %>% 
  group_by(Ingredient_code, food_V2_ID) %>%
  mutate(PRO_Rank = dense_rank(abs(FooDB_Value - ASA_Value))) %>%
  select(c(Ingredient_code, food_V2_ID, mean_score, text_rank, PRO_Rank))
```

Let us recombine Macros for Assessment
```{r message = FALSE}
scores = finaloutput.updated %>%
  left_join(CHO) %>%
  left_join(PRO) %>%
  left_join(FAT)

# FooDB has more CHO data available than on PRO or FAT. 
scores = scores %>%
  mutate(AvgTextCHO_Rank = rowMeans(select(scores, c(text_rank, CHO_Rank))))
```


#### 3b) Filtering

Check which Text and CHO Rank Averages are missing. (If CHO rank is missing, we also do not have a rank for FAT and PRO.)
```{r}
NA.Macros = scores %>%
  filter(is.na(AvgTextCHO_Rank))

message('Number of unique codes that do not have macronutrient data: ', length(unique(NA.Macros$Ingredient_code)))
```

Let us grab the rest of the entries associated with those Ingredient codes with no macronutrient information so we can identify the best entry based on text similarity

```{r}
#In codes with no macronutrient data, filter based off highest text similarity
NA.Macros.hits = scores %>% 
  filter(Ingredient_code %in% NA.Macros$Ingredient_code) %>%
  group_by(Ingredient_code) %>%
  filter(mean_score == max(mean_score, na.rm=TRUE)) %>%
  distinct(mean_score, .keep_all = TRUE)
```


```{r}
#In codes with macronutrient data, filter first on average text/CHO rank
#then filter second on highest mean text similarity score
Macros.hits = scores %>% 
  filter(!Ingredient_code %in% NA.Macros$Ingredient_code) %>%
  group_by(Ingredient_code) %>%
  filter(AvgTextCHO_Rank == min(AvgTextCHO_Rank, na.rm=TRUE)) %>%
  filter(mean_score == max(mean_score)) %>%
  distinct(mean_score, .keep_all = TRUE)

message('Number of unique codes that have macronutrient data: ', length(unique(Macros.hits$Ingredient_code)))
```

#### 3c) Merge and Export

```{r message = FALSE}
#Merge the two lists with their new matches 
greplhits = full_join(NA.Macros.hits, Macros.hits)
```

```{r message = FALSE}
#Merge the previously combined list with the perfect matches
#Add descriptor terms for a second search approach. 
missing.matches = missing %>%
  left_join(perfectmatch) %>%
  filter(!Ingredient_code %in% greplhits$Ingredient_code) %>%
  full_join(greplhits)

write.csv(missing.matches, 'data/missing_foodb_descrip.csv', row.names = FALSE)
```


