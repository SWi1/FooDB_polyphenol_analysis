{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching FooDB to ASA24 Ingredient Descriptions\n",
    "## Step 5: Matching ASA24 to FooDB\n",
    "### Part 2: Ingredient Description Dependency Parsing for missing code matches\n",
    "\n",
    "__Required Input Files__\n",
    "\n",
    "  - *asa_descripcleaned_codematched.csv* - Output from 04_FooDB_FullMatch_Part1, all ASA24 ingredient descriptions from FL100\n",
    "\n",
    "__Information__  \n",
    "This script runs a natural language processing algorithm on ASA24 ingredient descriptions from the FL100 study. \n",
    "\n",
    "    1) Apply nlp to each row and examine parts of speech, tags, and dependencies for ingredient tokens.\n",
    "    2) Dependency parsing and add columns to food description dataframe.\n",
    "\n",
    "\n",
    "__Output__\n",
    "  \n",
    "  - *asa_foodb_descrip_dependencies.csv* - Input data but with dependency token columns added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/stephanie.wilson/Desktop/SYNC/Scripts/FooDB_FNDDS'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensure working directory is the project folder\n",
    "mapping = os.getcwd()\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract observations that are missing foodB descriptions\n",
    "asa = pd.read_csv('data/asa_descripcleaned_codematched.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Apply nlp to each row, and examine POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply nlp function on each food description that will be eventualkly searched against FooDB\n",
    "asa_nlp = asa['Ingredient_description'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we want to see all attributes from nlp features, we can run *dir*() on the first row of descriptions\n",
    "dir(asa_nlp[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    " > dir(asa_nlp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: frozen \n",
      "Part of Speech: ADJ \n",
      "Tag: JJ , adjective (English), other noun-modifier (Chinese) \n",
      "Dependency: amod , adjectival modifier \n",
      "\n",
      "Token: novelties \n",
      "Part of Speech: NOUN \n",
      "Tag: NNS , noun, plural \n",
      "Dependency: compound , compound \n",
      "\n",
      "Token: juice \n",
      "Part of Speech: NOUN \n",
      "Tag: NN , noun, singular or mass \n",
      "Dependency: compound , compound \n",
      "\n",
      "Token: type \n",
      "Part of Speech: NOUN \n",
      "Tag: NN , noun, singular or mass \n",
      "Dependency: compound , compound \n",
      "\n",
      "Token: orange \n",
      "Part of Speech: NOUN \n",
      "Tag: NN , noun, singular or mass \n",
      "Dependency: ROOT , root \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine parts of speech from the first food description\n",
    "for tok in asa_nlp[25]:\n",
    "    pos = tok.pos_ #coarse-grain POS\n",
    "    tag = tok.tag_ #fine-grain POS\n",
    "    dep = tok.dep_ #word dependency\n",
    "    print(\n",
    "    'Token:', tok.text,\n",
    "    '\\nPart of Speech:', pos,\n",
    "    '\\nTag:', tag, \",\", spacy.explain(tag),\n",
    "    '\\nDependency:', dep, \",\", spacy.explain(dep),\n",
    "    \"\\n\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Dependency Parsing of Ingredient Descriptions\n",
    "\n",
    "Create functions to pull out specific dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get roots\n",
    "def get_root(doc):    \n",
    "    DEPdata = []\n",
    "\n",
    "    for tok in doc:\n",
    "        dep = tok.dep_\n",
    "        if dep == 'ROOT':\n",
    "            DEPdata.append(tok)\n",
    "    return(DEPdata)  \n",
    "    \n",
    "# get compound\n",
    "def get_compound(doc):    \n",
    "    DEPdata = []\n",
    "\n",
    "    for tok in doc:\n",
    "        dep = tok.dep_\n",
    "        if dep == 'compound':\n",
    "            DEPdata.append(tok)\n",
    "    return(DEPdata)  \n",
    "\n",
    "# get nominal subjects\n",
    "def get_nsubj(doc):    \n",
    "    DEPdata = []\n",
    "\n",
    "    for tok in doc:\n",
    "        dep = tok.dep_\n",
    "        if dep == 'nsubj':\n",
    "            DEPdata.append(tok)\n",
    "    return(DEPdata)    \n",
    "\n",
    "# get adjectival modifier \n",
    "def get_amod(doc):    \n",
    "    DEPdata = []\n",
    "\n",
    "    for tok in doc:\n",
    "        dep = tok.dep_\n",
    "        if dep == 'amod':\n",
    "            DEPdata.append(tok)\n",
    "    return(DEPdata)  \n",
    "\n",
    "# get noun modifier \n",
    "def get_nmod(doc):    \n",
    "    DEPdata = []\n",
    "\n",
    "    for tok in doc:\n",
    "        dep = tok.dep_\n",
    "        if dep == 'nmod':\n",
    "            DEPdata.append(tok)\n",
    "    return(DEPdata)  \n",
    "\n",
    "# get noun phrase as adverbial modifier \n",
    "def get_npadvmod(doc):    \n",
    "    DEPdata = []\n",
    "\n",
    "    for tok in doc:\n",
    "        dep = tok.dep_\n",
    "        if dep == 'npadvmod':\n",
    "            DEPdata.append(tok)\n",
    "    return(DEPdata)  \n",
    "\n",
    "# get nominal subject (passive) \n",
    "def get_nsubjpass(doc):    \n",
    "    DEPdata = []\n",
    "\n",
    "    for tok in doc:\n",
    "        dep = tok.dep_\n",
    "        if dep == 'nsubjpass':\n",
    "            DEPdata.append(tok)\n",
    "    return(DEPdata)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was determined from examining the dependency output that tokens registering as a 'compound' accurately described the food, albeit generally. However, not all ingredient descriptions had a compound token, so an order needed to be established to see which dependencies were most important in describing food. \n",
    "\n",
    "The following order was manually found to optimize description accuracy.  \n",
    "  - compound > nsubjpass > nmod > nsubj > amod > npadvmod > ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the above function to all ingredient desecriptions descriptions\n",
    "asa_nlp_dep = pd.DataFrame(asa_nlp.apply(lambda x: get_compound(x)))\n",
    "asa_nlp_dep = asa_nlp_dep.rename(columns = {'Ingredient_description':'compound'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure order as listed above\n",
    "# Compound must be first\n",
    "asa_nlp_dep['nsubjpass'] = asa_nlp.apply(lambda x: get_nsubjpass(x))\n",
    "asa_nlp_dep['nmod'] = asa_nlp.apply(lambda x: get_nmod(x))\n",
    "asa_nlp_dep['nsubj'] = asa_nlp.apply(lambda x: get_nsubj(x))\n",
    "asa_nlp_dep['amod'] = asa_nlp.apply(lambda x: get_amod(x))\n",
    "asa_nlp_dep['npadvmod'] = asa_nlp.apply(lambda x: get_npadvmod(x))\n",
    "asa_nlp_dep['ROOT'] = asa_nlp.apply(lambda x: get_root(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['compound', 'nsubjpass', 'nmod', 'nsubj', 'amod', 'npadvmod', 'ROOT']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensure correct order\n",
    "list(asa_nlp_dep.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "asa_nlp_dep['Ingredient_description'] = asa['Ingredient_description']\n",
    "asa_updated = pd.merge(asa, asa_nlp_dep, on = 'Ingredient_description', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ingredient_code',\n",
       " 'Ingredient_description',\n",
       " 'orig_food_id',\n",
       " 'orig_food_common_name',\n",
       " 'food_V2_ID',\n",
       " 'compound',\n",
       " 'nsubjpass',\n",
       " 'nmod',\n",
       " 'nsubj',\n",
       " 'amod',\n",
       " 'npadvmod',\n",
       " 'ROOT']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(asa_updated.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the resulting description file with dependencies\n",
    "asa_updated.to_csv('data/asa_foodb_descrip_dependencies.csv', index = None, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7fb5e4ab2ecec768b3cdb5d6b68ea750b87930403d6f19d139ed99f98596544"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
